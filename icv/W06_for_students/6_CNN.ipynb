{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Classification of Small Images (CIFAR 10 dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an adaptation of a demo notebook in the Pytorch documentation. Please be patient while running the code in the notebook. \n",
    "\n",
    "We haven't used a GPU \\(that might be available on your computer\\) and training two epochs on CPU takes around a minute.\n",
    "\n",
    "In order to run the code you need to install pytorch on your system. The modules `torch` and `torchvision` should be installed. See the [PyTorch](https://pytorch.org/) website how to install Pytorch on your machine.\n",
    "\n",
    "*Important*: We recommend installing torch version <= 1.13.1.\n",
    "\n",
    "The [PyTorch](https://pytorch.org/) website is invaluable for understanding the code in this notebook. And it is a great source for further exploration of modern Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU used\n"
     ]
    }
   ],
   "source": [
    "device = None; \n",
    "# device = torch.device('cpu') # uncomment this line to explicitly set the device\n",
    "if device == None:\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA used\")\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        print(\"CPU used\")\n",
    "        device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Images from the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch includes several built-in datasets, including CIFAR. The provided code downloads and load this dataset, transforming the images during training and testing. Specifically, it converts images to tensors (PyTorch's fundamental data structure, similar to NumPy's ndarray) and normalizes them by subtracting 0.5 from each RGB value and dividing by 0.5. This shifts the RGB values from a range of 0-1 to -1 to +1, which is crucial for neural network performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch loads a dataset into separate train and test sets. First we define the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following 2 lines if you get a SSL certificate error to disable the verification.\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context (edited)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "print(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainset has an attribute ``data`` that contains the entire datamatrix in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. 50000 images of 32x32 pixels each with an R, G and B value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the labels are encoded in a simple Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values are indices from 0 to 9 and the corresponding class names are encoded in the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "classnames = trainset.classes\n",
    "print(classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pure Python/Numpy we can now display one of the images with the target class name as title for the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 66\n",
    "plt.imshow(trainset.data[i])\n",
    "plt.axis('off')\n",
    "plt.title(classnames[trainset.targets[i]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on we will define a ``DataLoader`` that makes it easy to load a small number of (image, target) tuples (a batch) needed in one gradient descent step in the stochastic gradient learning process. While loading the data the data in the dataset is transformed on the fly. For this dataset two transforms are used in sequence. First the ``ToTensor`` transform that takes an HxWxC (with C=3) numpy array into a PyTorch tensor of shape CxHxW which is the standard for encoding color images in deep learning. The second transform in the sequence is a normalization (see the PyTorch documentation for ``torchvision.transforms.Normalize``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will create a `DataLoader` to facilitate the loading of small batches of (image, target) tuples required for each step of stochastic gradient descent. This DataLoader will perform on-the-fly transformations on the dataset. For this dataset, two sequential transformations are applied: `ToTensor`, which converts an HxWxC (where C=3) NumPy array into a PyTorch tensor in the CxHxW format (standard for color images in deep learning), and normalization, as specified in the PyTorch documentation for `torchvision.transforms.Normalize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1.1**</span>\n",
    "\n",
    "Given the normalization ``Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))`` and an original color value (r,g,b) what is the normalized color value (r',g',b')? Why do we have 6 0.5s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "After normalizing the range shifts to -1 to +1. The Normalize takes the (mean, std) as inputs. in a range between 0-1, the mean and std are the same: 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we define and load the testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a `DataLoader` for both the training and testing sets. In the DataLoader's constructor, specify the batch_size. This DataLoader, functioning as a Python iterable, enables the retrieval of `batch_size` arbitrary images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Tensor Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For displaying 'tensor images' we provide two helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img, ax=None):\n",
    "    \"\"\"imshow for Tensor Images obtained with a DataLoader\n",
    "    from a data set. The images are of shape (3,M,N)\n",
    "    and are in the range from -1 to +1\"\"\"\n",
    "    img = (img - img.min()) / (img.max()-img.min())     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if ax==None:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    else:\n",
    "        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        \n",
    "def imtable(imgs, titles, ncols, hsize=4, vsize=4):\n",
    "    nrows = int(np.ceil(len(imgs)/ncols))\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(ncols*hsize, nrows*vsize))\n",
    "    r, c = 0, 0\n",
    "    for i, (im, label) in enumerate(zip(imgs, titles)):\n",
    "        if nrows==1:\n",
    "            ax = axs[c]\n",
    "        elif ncols==1:\n",
    "            ax = axs[r]\n",
    "        else:\n",
    "            ax = axs[r,c]\n",
    "        imshow(im, ax=ax)\n",
    "        ax.set_title(titles[i])\n",
    "        ax.axis('off')\n",
    "        c += 1\n",
    "        if c == ncols:\n",
    "            r += 1\n",
    "            c = 0\n",
    "    plt.tight_layout()\n",
    "        \n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "imtable(images, [classnames[i] for i in labels], 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1.2**</span>\n",
    "\n",
    "If you would be random guessing the class of an image what would be the expected accuracy of the 'classifier'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "The expected accuracy is 10 percent. Because the database has 10 different classes. The probablity for each class is 1/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We define a CNN to classify CIFAR images by first instantiating the functional processing blocks in the `__init__` function and then utilizing them in the `forward` method to implement the network's forward pass from input to output.\n",
    "\n",
    "PyTorch automatically handles the backpropagation to calculate derivatives of the loss with respect to the data throughout the network. However, if you introduce new functional processing blocks as a programmer, you will need to manually implement the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note in the forward pass we have 'numbered'\n",
    "        # the x's just to make it easier to collect\n",
    "        # the code to print information about the network\n",
    "        # further on in the code.\n",
    "\n",
    "        # This is very bad code!!\n",
    "\n",
    "        conv1_out = self.conv1(x)\n",
    "        if self.verbose == 1:\n",
    "            print(\"Shape of conv1(x):\", conv1_out.shape)\n",
    "        x1 = self.pool1(F.relu(self.conv1(x)))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x1: {x1.shape}\")\n",
    "        x2 = self.pool2(F.relu(self.conv2(x1)))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x2: {x2.shape}\")\n",
    "        x3 = torch.flatten(x2, 1) # flatten all dimensions except batch\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x3: {x3.shape}\")\n",
    "        x4 = F.relu(self.fc1(x3))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x4: {x4.shape}\")\n",
    "        x5 = F.relu(self.fc2(x4))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x5: {x5.shape}\")\n",
    "        x6 = self.fc3(x5)\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x6: {x6.shape}\")\n",
    "        self.conv1 = nn.Conv2d(3, 10, 5)\n",
    "        if self.verbose == 1:\n",
    "            print(self.conv1)\n",
    "\n",
    "        if self.verbose == 1:\n",
    "            # Write the code to print the shapes of the \n",
    "            # different stages in the CNN\n",
    "            # YOUR CODE HERE (Replace this and the following line with your code)\n",
    "            self.verbose -= 1\n",
    "        \n",
    "        return x6\n",
    "\n",
    "# with verbose=1 the 'debugging' code is printed only once \n",
    "# while running the forward pass multiple times.\n",
    "net = Net(verbose=0).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print an instance of a network class, it provides an overview of the functional processing blocks defined within. Note that functions like `relu` and `flatten` won't appear in this printout, although they are part of the processing flow. The reason for this is that we define these 'activation' and 'flattening' functions as part of the forward pass, utilizing `functional` block, and not `nn.Module` representing operation (see [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) and [torch.nn.functional.relu\n",
    "](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html), and [this thread](https://discuss.pytorch.org/t/f-relu-and-nn-relu/123071/2) to understand the difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When you instantiate the `Net` class and print it, the output does not show the shapes of data processed at each layer. To address this, you can modify the `Net` class to include detailed information about the data shapes.\n",
    "\n",
    "The example code sets the `verbose` attribute of the `Net` instance to 1 and feeds it a batch of images. While the output is printed, it’s important to remember that it doesn't represent meaningful data yet as the network hasn't learned anything.\n",
    "\n",
    "PyTorch automatically handles the initialization of parameters for each processing module in the network. However, you can customize this initialization if desired by referring to the PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of conv1(x): torch.Size([4, 10, 28, 28])\n",
      "Shape x1: torch.Size([4, 10, 14, 14])\n",
      "Shape x2: torch.Size([4, 16, 5, 5])\n",
      "Shape x3: torch.Size([4, 400])\n",
      "Shape x4: torch.Size([4, 120])\n",
      "Shape x5: torch.Size([4, 84])\n",
      "Shape x6: torch.Size([4, 10])\n",
      "Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "tensor([[-0.1181, -0.0525,  0.0119, -0.0474, -0.0450,  0.0336,  0.0735, -0.0862,\n",
      "         -0.0229, -0.0993],\n",
      "        [-0.1317, -0.0487,  0.0318, -0.0410, -0.0477,  0.0438,  0.0615, -0.0867,\n",
      "         -0.0421, -0.1039],\n",
      "        [-0.1211, -0.0577,  0.0082, -0.0432, -0.0433,  0.0396,  0.0795, -0.0830,\n",
      "         -0.0347, -0.0911],\n",
      "        [-0.1194, -0.0524,  0.0202, -0.0476, -0.0450,  0.0322,  0.0796, -0.0759,\n",
      "         -0.0317, -0.0909]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net.verbose=1 # when you run it you should set this to 1\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "outputs = net(images)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the printed information you ought to be able to answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a data graph the first part of this network looks like\n",
    "<img src=\"cifar_cnn_incomplete.png\"\n",
    "     width=80%\n",
    "     alt=\"data flow graph\"\n",
    "     style=\"float: center;\" />\n",
    "\n",
    "In a typical CNN representation, the **data blocks** are depicted as the dimensions of the data at each stage, with **arrows** representing the processing modules. For instance, the first arrow might represent the sequence of operations: `Conv1 --> ReLU --> Maxpool`.\n",
    "\n",
    "The CIFAR images start as 32x32 pixels with 3 color channels (RGB), denoted as $W_1 = H_1 = 32$ and $C_1 = 3$. At the end of the CNN, the output is a 10-element vector, where each element corresponds to the activation for one of the ten classes. This final layer is achieved through a fully connected network that maps from a previous layer, which is an 84-element vector.\n",
    "\n",
    "To track changes in data shape after specific layers, such as after the first or second convolution, you would need to adjust the `forward` method in the network class. This modification can help with debugging or understanding how the network processes and transforms the input data through its layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2.1**</span>\n",
    "\n",
    "If you print the shape of tensor ``x`` in the ``forward`` method you will get ``torch.Size([4, 3, 32, 32])``. Why is the first dimension of size 4 in this tensor? Note that this size is NOT depicted in the above graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "The first element of the list is the batch size. Here it's 4 beacuse we want to take batches of 4 pictures from the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2.2**</span>\n",
    "\n",
    "What is the shape of the output of ``conv1(x)``? What does it tell you about the convolution/correlation algorithm that is used? To find this experimentally you have to change the code in the ``forward`` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "After changing the forward function the shape of conv1(x) is: torch.Size([4, 10, 28, 28]) the shrinking height and width indicate that convolution is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2.3**</span>\n",
    "\n",
    "Complete the data graph. For each of data block (image blocks at the start and flattened arrays at the end) you should indicate the shape of the block. The blocks correspond with the tensors `x`, `x1` ... `x6` in the `forward` method. Either you should look into the documentation of the processing modules that are being used in the `__init__` function or you should run the code and print the shapes (when we are running/training the network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "after changing the documentation and printing the shapes the shapes are:\n",
    "Shape x1: torch.Size([4, 10, 14, 14])\n",
    "Shape x2: torch.Size([4, 16, 5, 5])\n",
    "Shape x3: torch.Size([4, 400])\n",
    "Shape x4: torch.Size([4, 120])\n",
    "Shape x5: torch.Size([4, 84])\n",
    "Shape x6: torch.Size([4, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2.4**</span>\n",
    "\n",
    "In the `__init__` function the following processing modules are defined: `conv1`, `pool1`, `conv2`, `pool2`, `fc1`, `fc2` and `fc3`. Indicate where these modules are used in your data flow graph (i.e. you have to label the arrows in the graph with the (sequence of) operations associated with each arrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "Vanuit de input laag naar naar de eerstvolgende laag (pijl 1 / fc1 -> fc2) is de sequence of operations: conv1 -> pool1. Daarna heeft pijl 2 (fc2 -> fc3) de sequence: conv2 -> pool2 -> fc3.\\\n",
    "Of in totaal: fc1 -> conv1 -> pool1 -> fc2 -> conv2 -> pool2 -> fc3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2.5**</span>\n",
    "\n",
    "For each of processing modules `conv1`, `pool1`, `conv2`, `pool2`, `fc1`, `fc2` and `fc3` give the number of parameters that have to be learned (don't forget the bias parameters...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "Om te beginnen hebben pooling layers geen trainbare parameters, dus die zal ik ook niet bespreken\\\n",
    "fc1: (120 x 400) + 120 = 48.120\\\n",
    "fc2: (84 x 120) + 84 = 10.164\\\n",
    "fc3: (10 x 84) + 10 = 850\\\n",
    "conv1: (10 x 3 x 5 x 5) + 10 = 760\\\n",
    "conv2: (16 x 10 x 5 x 5) + 16 = 4.016\\\n",
    "Geeft een totaal van 63.910 trainbare parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT**: The answer to question 2.5 can be computed by 'interrogating' the network. For instance for the `fc2` linear module we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 5, 5])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for p in net.conv2.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for this module there are two tensors defining the parameters. Note that an object of type `torch.Size` is not a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2.6**</span>\n",
    "\n",
    "What are these tensors? (please look back in your machine learning course notes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "A tensor is a multi-dimensional array that in machine learning efficiently represent data and model parameters for computation for CPUs or GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2.7**</span>\n",
    "\n",
    "How many parameters in total for this `fc2` module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "10.164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you have to write the code to calculate the total number of parameters to be learned in this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 weights: torch.Size([10, 3, 5, 5]) = 750\n",
      "conv1 bias: torch.Size([10]) = 10\n",
      "conv2 weights: torch.Size([16, 10, 5, 5]) = 4000\n",
      "conv2 bias: torch.Size([16]) = 16\n",
      "fc1 weights: torch.Size([120, 400]) = 48000\n",
      "fc1 bias: torch.Size([120]) = 120\n",
      "fc2 weights: torch.Size([84, 120]) = 10080\n",
      "fc2 bias: torch.Size([84]) = 84\n",
      "fc3 weights: torch.Size([10, 84]) = 840\n",
      "fc3 bias: torch.Size([10]) = 10\n",
      "Total number of parameters in the network: 63910\n"
     ]
    }
   ],
   "source": [
    "# Write the code to calculate the total number of parameters in the entire network\n",
    "# YOUR CODE HERE (Replace this and the following line with your code)\n",
    "total_params = 0\n",
    "for p, name in zip(net.parameters(), ['conv1 weights', 'conv1 bias', 'conv2 weights', 'conv2 bias', 'fc1 weights', 'fc1 bias', 'fc2 weights', 'fc2 bias', 'fc3 weights', 'fc3 bias']):\n",
    "    print(f'{name}: {p.shape} = {p.numel()}')\n",
    "    total_params += p.numel()\n",
    "print(\"Total number of parameters in the network:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on in this notebook we revisit the interrogation of the network again to actually observe what parameter values are learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network we also need to define a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When input data is processed through a neural network in PyTorch, the output is obtained with:\n",
    "\n",
    "```python\n",
    "outputs = net(inputs)\n",
    "```\n",
    "\n",
    "These outputs are then compared with the target values using the `CrossEntropyLoss` function. This loss function is particularly suited for classification tasks because it handles the softmax activation internally, which is why a separate softmax function is not required in this network.\n",
    "\n",
    "It is important to note that while the network output is a 10-element vector representing the activation for each class, the target for each example is simply an integer representing the class ID. The conversion from this integer format to a _one-hot encoding_ is handled implicitly within the `CrossEntropyLoss` function, simplifying the input requirements for this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an optimizer. A standard optimizer is PyTorch is the SGD (Stochastic Gradient Descent) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, dampening=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here `lr` is the learning rate. The momentum is used in the gradient descent procedure to 'remember' in which direction the gradient descent is running and not deviate too easily from that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2.8**</span>\n",
    "\n",
    "Read the explanation of the SGD optimizer in the PyTorch documentation.\n",
    "1. Why the name **stochastic** gradient descent?\n",
    "2. What value for the ``momemtum`` and `dampening` should be chosen to get the classical gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\\\n",
    "Stochastic means that the optimizer estimates the gradient instead of calculating the gradient over the whole dataset at each step. The estemating introduces noise.\\\n",
    "To get classic gradient descent, the values should be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network using a variant of the gradient descent algorithm typically involves the following steps:\n",
    "\n",
    "1. **Feed a Mini-Batch**: Feed a mini-batch of examples into the network.\n",
    "2. **Forward Pass**: Execute the forward pass, which calculates the output through all the network's intermediate processing modules.\n",
    "3. **Zero Gradients**: Set all gradients to zero to prevent accumulation from previous iterations.\n",
    "4. **Backward Pass**: Calculate the backward pass to determine all the gradients.\n",
    "5. **Parameter Update**: Update each parameter by making a step in the direction that minimizes the loss.\n",
    "\n",
    "This process is repeated for all mini-batches in the training set, completing what is known as an **epoch**. The training continues for a specified number of epochs.\n",
    "\n",
    "During training, it's important to monitor the loss:\n",
    "- **Training Loss**: Calculate and record the loss on the training set after each epoch.\n",
    "- **Test Loss**: Similarly, calculate the loss on the test set to evaluate generalization.\n",
    "- **Intermediate Monitoring**: To check progress within an epoch, report the loss for the last few batches.\n",
    "\n",
    "The entire process for one epoch is encapsulated in the `train_loop` function, which handles these operations and tracks progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, no_report=1000, device=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move the data to GPU (if available)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % no_report == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While learning for several epochs we also calculate the performance on the test or validation set. This is encoded in the function `test_loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, device=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning process then becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.260852  [    0/50000]\n",
      "loss: 2.292311  [ 4000/50000]\n",
      "loss: 2.264485  [ 8000/50000]\n",
      "loss: 2.311833  [12000/50000]\n",
      "loss: 2.328743  [16000/50000]\n",
      "loss: 2.399134  [20000/50000]\n",
      "loss: 2.254781  [24000/50000]\n",
      "loss: 2.291280  [28000/50000]\n",
      "loss: 2.251230  [32000/50000]\n",
      "loss: 2.450566  [36000/50000]\n",
      "loss: 2.310879  [40000/50000]\n",
      "loss: 2.423047  [44000/50000]\n",
      "loss: 2.304096  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 18.6%, Avg loss: 2.192839 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.229520  [    0/50000]\n",
      "loss: 2.230584  [ 4000/50000]\n",
      "loss: 2.009138  [ 8000/50000]\n",
      "loss: 2.044782  [12000/50000]\n",
      "loss: 2.002610  [16000/50000]\n",
      "loss: 1.952102  [20000/50000]\n",
      "loss: 1.747257  [24000/50000]\n",
      "loss: 2.499846  [28000/50000]\n",
      "loss: 1.763695  [32000/50000]\n",
      "loss: 2.188468  [36000/50000]\n",
      "loss: 2.631223  [40000/50000]\n",
      "loss: 1.463880  [44000/50000]\n",
      "loss: 2.067712  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 28.1%, Avg loss: 1.975714 \n",
      "\n",
      "Done!\n",
      "1min 48s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(trainloader, net, loss_criterion, optimizer, device=device)\n",
    "    test_loop(testloader, net, loss_criterion, device=device)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following 2 lines to save your network to file\n",
    "# PATH = './cifar_net_100e.pth'\n",
    "# torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# when saved you can load it back into memory with\n",
    "# (again uncomment the following 2 lines)\n",
    "# net = Net().to(device)\n",
    "# net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load some images from the test set, display these image with their ground truth label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_MultiProcessingDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dataiter = \u001b[38;5;28miter\u001b[39m(testloader)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m images, labels = \u001b[43mdataiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m()\n\u001b[32m      3\u001b[39m imtable(images, [classnames[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m labels], \u001b[32m4\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: '_MultiProcessingDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imtable(images, [classnames[i] for i in labels], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we feed these to the network and print the outputs of the 10 output nodes for all images. Note that now we are not training the network, so torch doesn't need to keep track of all the intermediate outputs of all modules in the network (in preparation of a backward pass) and we can tell Pytorch not to do that with `no_grad()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training phase, the `softmax` function was not applied to the outputs of the network. This is because the `CrossEntropyLoss` function in PyTorch automatically includes softmax as part of its computation, and using an additional softmax layer before applying this loss function is not necessary and can lead to incorrect training results.\n",
    "\n",
    "However, when using the network in feed-forward mode only (i.e., for inference and not training), it is appropriate to apply the `softmax` function to the outputs. This normalizes the outputs to produce a distribution of probabilities across the 10 classes, representing the model's predictions as a posteriori class probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17115887 0.10935178 0.07656749 0.05435533 0.10161292 0.08026578\n",
      "  0.01617444 0.20559044 0.1046323  0.08029062]\n",
      " [0.1415459  0.41484976 0.01179793 0.00971141 0.02468218 0.00740919\n",
      "  0.00384781 0.01180584 0.21086945 0.16348055]\n",
      " [0.04747098 0.01228911 0.19191834 0.15603995 0.18090376 0.11475608\n",
      "  0.16525981 0.07525932 0.0363613  0.01974124]\n",
      " [0.07188392 0.20569275 0.04209489 0.07738608 0.05358511 0.0690819\n",
      "  0.04197348 0.13072686 0.08316759 0.22440737]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = F.softmax(net(images.to(device)), dim=-1)\n",
    "npoutputs = outputs.cpu().numpy()\n",
    "print(npoutputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted label will be the label with the largest output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     :       horse   airplane       bird      truck\n",
      "Predicted:       horse automobile       bird      truck\n"
     ]
    }
   ],
   "source": [
    "maxvalue, maxindex = torch.max(outputs, 1)\n",
    "print('True     : ', ' '.join('%10s' % classnames[labels[j]]\n",
    "                              for j in range(batch_size)))\n",
    "print('Predicted: ', ' '.join('%10s' % classnames[maxindex[j]]\n",
    "                              for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a small set of test images we run into the possibility that the classification is very bad. Let's test the network more carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the internal workings of a convolutional neural network (CNN) is often complex. Specifically, it can be challenging to determine what the learned weights represent, and many aspects of these networks remain largely unexplained. While there are several research papers that attempt to describe what specific nodes in a network respond to, these analyses are typically beyond the scope of an introductory course.\n",
    "\n",
    "### Visualizing the First Layer in CNNs\n",
    "\n",
    "An exception to this complexity is the visualization of the first layer in a CNN. The first layer's convolutional kernels directly interact with the input image data and can be visualized as images. In traditional computer vision, convolution kernels are used to detect specific features like edges, corners, and textures, and these can be similarly interpreted in CNNs.\n",
    "\n",
    "The hypothesis behind CNNs is that local structures in images carry the spatial information necessary for image interpretation. By visualizing the kernels, we can see if the network has learned to detect various local structure patterns, such as edges and textures. These patterns are then combined in subsequent layers to recognize larger structures in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to visualize the kernels in the first convolution module `conv1` as small images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAE3AAAAHqCAYAAADrfmgQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMb5JREFUeJzs3GvMnwV5x/G7TXEyFREmyCZMhhWmiJyE4ThUCg3QhmUKcrCRTMECJVgOAh2kihuCsKJMOgVqLYyD0roxEBisMlg1IqdFjgMlrErQTQoCMqBd7V70NeH3pveVcX0+r+/k+qVpnuf5///P852wdu3atQMAAAAAAAAAAAAAAAAAAAAAAAAAr2li9QAAAAAAAAAAAAAAAAAAAAAAAACA/y8E3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEJqUP/ttR56/PHbyKyZevqZ7Q0vd+sk31hJbuevCX1RNa+tXX7q2e0NI1t14++s2t9/706DcZhp/PP7J6Qkszp+gUV5j2T77OVDh3j1OrJ7T0wJuOHvXepA1PG/Ue6+x3ivdCKsw/b0b1hJZ2W76kekJLm73lG9UTWnpi+xNGvzn5lDNHv8kwfOpt76me0NJRG99UPaGlP79sz+oJLR3+5V9VT2hpzr5fGPXeKTOOHfUe61z6/qerJ7T0vV13qZ7Q0iG7Plc9oaUJ01dWT2hpxYOXjX7zkhufHP0mw/Dt6Q9WT2hp7sytqye0dPaCJ6ontPSv52xWPaGlDc7fedR7t94xZdR7rHPQ/HHfe2Gd5zb+bfWElpbNXFQ9oaVrH35D9YSWrpqzcPSb29x3/eg3GYbDPuBz8go//vqC6gktzbvzouoJLd00z2vyCmdPHvfz0723mD3qPdZZcsYx1RNaOvmgnaontLTjl/apntDS1pvfUz2hpUPO+c3oNz938HWj32QYdn56avWElj4/1ee1FaadOa96QkvT5k+pntDS1DPnvOYzyhYAAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQmpQ++NWV567PHbyKxzbbrXpCS7N/uLh6Qkv/st0e1RNaWnDJDtUTGMkbZ7ynekJLJ919VfWElpaetLp6QkvbHHpy9YSWnt/zS9UTerrh6FHPHbDx/FHvsc63bn6iekJLx576UvWEll659dHqCS29f9mL1RN6Wj7+ydP/c8H4Rxme/8BPqie0tMWx+1VPaGnFyz+qntDSdzd5qnpCS3NGvvf7f7Zo5IsMwzBMu+Yz1RNaeu/02dUTWnrwiGnVE1ra8ZvbVk9gJCv2+MfqCS199KSdqie0tOzBz1dPaOmB61ZWT2hp1opfVE9oadHwwKj3Vt988Kj3WOemu7zHW2GHe0+pntDSJld9s3pCS5/c+bzqCYxkr11eqZ7Q0iN/9HD1hJZ2O+Y71RNamnz0J6sntLT7IR+rntDTj8c9d+C7rh/3IMMwDMOj87y3XuHDJ1xbPaGlz36lekFPf7DBp6ontHRIwc0tZy0puMrJkydUT2hpu138rUaFiXfvVT2hpU1u1K2oMec1n5i4/kcAAAAAAAAAAAAAAAAAAAAAAAAAvD4IuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgNCk9MFnt9xrfe7gVdy/z/LqCS399OhXqie0dPyq46ontPQ7n9i1ekJPs8Y/OWHeXeMfZVh2wpLqCS3N3P3J6gktvfPRK6sntDTnT6ZWT2AE+y49v3pCS7P2/Fb1hJbmHrJZ9YSW7n7nkdUTWjrsg9+unsBI/vgrd1RPaOnqqWdVT2jpwulvrp7Q0uwHfE+pcMbU56sn9LTy3FHPPXvtL0e9xzoP3LqmekJL35/mvZAKd19zSvWElj53uNdpJX44/sm9Vx8x/lGG5y+YWT2hpRcW+hmqwqS1f109oaVzPvZ49QRGcNtfnVE9oaWzFnptWGHzD21UPaGl1cfvVz2hpS3f+oPqCU2dOfrFO+59x+g3GYY7PnFZ9YSWLr3qT6sntLT48K9VT2jp19Mfqp7Q0kEj33vkR1uMfJFhGIbfe9+L1RNauugv/H+v8MSL/q65wqan/bZ6Qk+7j3/ykSU3jH+U4fY1t1VPaOmSbf0MVeGca7w2rPDkMX4vpMLfL3rtZyau/xkAAAAAAAAAAAAAAAAAAAAAAAAArw8CbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAIDQpffCZvS9dnzt4FdOnrq2e0NI9f7u0ekJLKzY7uXpCS2df8rvVE1p6etZzo9/82c4njH6TYbjobz5cPaGlT9+8W/WElh7+uwOrJ7T01WM2qp7ACLY9+8nqCS1dfNL91RNamvXCtdUTWtrqxHdUT2jpoR0mVE9gJEdveFz1hJauvu6G6gktPfP9adUTWrplqfdCKpz1wibVE1o6b+R7kzaeOfJFhmEYjprhvZAKtzx1YvWElq64e+yvbAzDMGx61H3VE1o6arh49Jurnnr76DcZhgP3+kb1hJaWrnhD9YSW7tvU15kKW10+u3pCS6uHI0e999Zrp496j3UeWnlo9YSWTtvX54YVtnvzd6sntPQPixZVT2hpxgfHv/n4ncvHP8pw4dzbqye09LOPe01e4fSP7Fs9oaWVf3hA9QRGcMUXd6ye0NI99767ekJLtx88o3pCS5ees6p6QkvHHnl19YSeHh//5LuGfx7/KMNL//W/1RNa+suvv1w9oaVlLzxdPaGlCYv9u5cIPtaYuP5XAAAAAAAAAAAAAAAAAAAAAAAAALw+CLgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAAAAABASMANAAAAAAAAAAAAAAAAAAAAAAAAICTgBgAAAAAAAAAAAAAAAAAAAAAAABAScAMAAAAAAAAAAAAAAAAAAAAAAAAICbgBAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAAAAEICbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAIDQpPTBebf+x/rcwat4easbqye0dOAFV1ZPaOmZ925aPaGlfd7+0+oJjGTizCnVE1pauMfm1RNaetPi7asntLT0rsurJ7S0YNXc6gktXTzyvS/s/5uRLzIMw7DlnR+qntDSfy/9QfWElv5n+1XVE1o6c82u1ROaGv/1wlMX7zL6TYZhypUrqye09NgtG1RPaOmeLQ+vntDSYZevrp7Q0+xxzx332P7jHmQYhmF42zM7VE9oaaPPPF09oaWVS+6tntDShl/0daaLZ7e9rXpCS/u/z3shFU4/3ueGFT6y5oLqCS3dfOFL1RN6Gvkl8mf//bFxDzIMwzAc8es11RNaWr7nlOoJLV2zYHr1hJbmTv549QRGMv/2FdUTWrp+kd+9qnD94vurJ7Q0+Rd+P6HCptvtWz2hpYdGvrd85c9HvsgwDMMBr/h+UmHSl5dVT2hp1XcOrZ7Q0sJtP1o9gZGc+spO1RNauurIg6sntHTnhWdXT2jpisULqye0tPItfnatceJrPjFxhBUAAAAAAAAAAAAAAAAAAAAAAAAArwsCbgAAAAAAAAAAAAAAAAAAAAAAAAAhATcAAAAAAAAAAAAAAAAAAAAAAACAkIAbAAAAAAAAAAAAAAAAAAAAAAAAQEjADQAAAAAAAAAAAAAAAAAAAAAAACAk4AYAAAAAAAAAAAAAAAAAAAAAAAAQEnADAAAAAAAAAAAAAAAAAAAAAAAACAm4AQAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAABCAm4AAAAAAAAAAAAAAAAAAAAAAAAAIQE3AAAAAAAAAAAAAAAAAAAAAAAAgJCAGwAAAAAAAAAAAAAAAAAAAAAAAEBIwA0AAAAAAAAAAAAAAAAAAAAAAAAgJOAGAAAAAAAAAAAAAAAAAAAAAAAAEBJwAwAAAAAAAAAAAAAAAAAAAAAAAAgJuAEAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAAAQgJuAAAAAAAAAAAAAAAAAAAAAAAAACEBNwAAAAAAAAAAAAAAAAAAAAAAAICQgBsAAAAAAAAAAAAAAAAAAAD8Xzt3UAMACAMwMODf87DQ30Jyp6AKCgAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEB0Zma2IwAAAAAAAAAAAAAAAAAAAAAAAAB+cLcDAAAAAAAAAAAAAAAAAAAAAAAAAH5h4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAEBk4AYAAAAAAAAAAAAAAAAAAAAAAAAQGbgBAAAAAAAAAAAAAAAAAAAAAAAARAZuAAAAAAAAAAAAAAAAAAAAAAAAAJGBGwAAAAAAAAAAAAAAAAAAAAAAAED0ADK/qosdc/t5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 5000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here the code to visualize the kernels\n",
    "# YOUR CODE HERE (Replace this and the following line with your code)\n",
    "def show_kernels(kernels, ncols, hsize, vsize):\n",
    "    fig, axs = plt.subplots(1, ncols, figsize=(ncols*hsize, vsize))\n",
    "    for i, kernel in enumerate(kernels):\n",
    "        ax = axs[i]\n",
    "        imshow(kernel, ax=ax)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "kernels = net.conv1.weight.data.cpu()\n",
    "n_colls = kernels.shape[0]\n",
    "h_size = kernels.shape[2]\n",
    "v_size = kernels.shape[3]\n",
    "show_kernels(kernels, ncols=n_colls, hsize=h_size, vsize=v_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right architecture for a convolutional neural network (CNN) often involves more art than science. Unlike other engineering disciplines where decisions can be based on precise calculations or established theories, designing a CNN requires a more experimental approach.\n",
    "\n",
    "The process of selecting and refining a CNN architecture typically involves a lot of trial and error. Developers/researchers might start with a basic model, test its performance, and then iteratively adjust the number of layers and channels to find the optimal configuration for their specific application. This experimental process allows for discovering how different configurations impact learning and performance, guiding the development towards the most effective model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.828965  [    0/50000]\n",
      "loss: 1.544151  [ 4000/50000]\n",
      "loss: 1.885576  [ 8000/50000]\n",
      "loss: 1.514337  [12000/50000]\n",
      "loss: 1.394904  [16000/50000]\n",
      "loss: 1.525707  [20000/50000]\n",
      "loss: 2.208327  [24000/50000]\n",
      "loss: 1.933784  [28000/50000]\n",
      "loss: 1.911335  [32000/50000]\n",
      "loss: 2.184885  [36000/50000]\n",
      "loss: 1.441190  [40000/50000]\n",
      "loss: 2.348816  [44000/50000]\n",
      "loss: 2.100231  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 1.894000 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.458751  [    0/50000]\n",
      "loss: 2.025425  [ 4000/50000]\n",
      "loss: 3.480224  [ 8000/50000]\n",
      "loss: 1.459165  [12000/50000]\n",
      "loss: 1.422562  [16000/50000]\n",
      "loss: 1.998060  [20000/50000]\n",
      "loss: 1.799861  [24000/50000]\n",
      "loss: 2.073916  [28000/50000]\n",
      "loss: 2.028346  [32000/50000]\n",
      "loss: 2.856667  [36000/50000]\n",
      "loss: 1.754584  [40000/50000]\n",
      "loss: 2.111237  [44000/50000]\n",
      "loss: 1.897576  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.844186 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.094272  [    0/50000]\n",
      "loss: 1.620717  [ 4000/50000]\n",
      "loss: 1.442888  [ 8000/50000]\n",
      "loss: 2.688902  [12000/50000]\n",
      "loss: 1.325908  [16000/50000]\n",
      "loss: 2.035292  [20000/50000]\n",
      "loss: 1.663310  [24000/50000]\n",
      "loss: 2.262302  [28000/50000]\n",
      "loss: 1.799402  [32000/50000]\n",
      "loss: 1.702755  [36000/50000]\n",
      "loss: 1.959408  [40000/50000]\n",
      "loss: 2.005211  [44000/50000]\n",
      "loss: 2.213569  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.854005 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.355169  [    0/50000]\n",
      "loss: 1.243963  [ 4000/50000]\n",
      "loss: 1.726472  [ 8000/50000]\n",
      "loss: 1.876168  [12000/50000]\n",
      "loss: 1.853510  [16000/50000]\n",
      "loss: 1.620304  [20000/50000]\n",
      "loss: 1.763208  [24000/50000]\n",
      "loss: 1.671715  [28000/50000]\n",
      "loss: 2.519915  [32000/50000]\n",
      "loss: 1.135048  [36000/50000]\n",
      "loss: 1.976054  [40000/50000]\n",
      "loss: 1.943329  [44000/50000]\n",
      "loss: 1.919323  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 1.801329 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.926867  [    0/50000]\n",
      "loss: 2.478324  [ 4000/50000]\n",
      "loss: 2.005762  [ 8000/50000]\n",
      "loss: 1.654629  [12000/50000]\n",
      "loss: 1.403097  [16000/50000]\n",
      "loss: 1.524853  [20000/50000]\n",
      "loss: 0.894564  [24000/50000]\n",
      "loss: 1.947074  [28000/50000]\n",
      "loss: 2.335548  [32000/50000]\n",
      "loss: 1.650186  [36000/50000]\n",
      "loss: 2.343135  [40000/50000]\n",
      "loss: 1.696007  [44000/50000]\n",
      "loss: 1.681573  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 1.782853 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.836995  [    0/50000]\n",
      "loss: 1.786396  [ 4000/50000]\n",
      "loss: 1.716908  [ 8000/50000]\n",
      "loss: 2.042753  [12000/50000]\n",
      "loss: 1.539745  [16000/50000]\n",
      "loss: 1.545685  [20000/50000]\n",
      "loss: 1.903950  [24000/50000]\n",
      "loss: 1.855174  [28000/50000]\n",
      "loss: 1.299398  [32000/50000]\n",
      "loss: 1.182298  [36000/50000]\n",
      "loss: 0.915128  [40000/50000]\n",
      "loss: 1.665691  [44000/50000]\n",
      "loss: 1.884161  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 1.748090 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.677817  [    0/50000]\n",
      "loss: 1.926743  [ 4000/50000]\n",
      "loss: 0.990478  [ 8000/50000]\n",
      "loss: 1.774528  [12000/50000]\n",
      "loss: 1.857433  [16000/50000]\n",
      "loss: 1.750299  [20000/50000]\n",
      "loss: 2.380580  [24000/50000]\n",
      "loss: 2.002639  [28000/50000]\n",
      "loss: 2.275343  [32000/50000]\n",
      "loss: 1.498032  [36000/50000]\n",
      "loss: 1.379499  [40000/50000]\n",
      "loss: 2.642525  [44000/50000]\n",
      "loss: 1.862891  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 1.715356 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.276492  [    0/50000]\n",
      "loss: 2.303618  [ 4000/50000]\n",
      "loss: 1.608772  [ 8000/50000]\n",
      "loss: 1.837403  [12000/50000]\n",
      "loss: 1.300033  [16000/50000]\n",
      "loss: 0.739401  [20000/50000]\n",
      "loss: 2.500526  [24000/50000]\n",
      "loss: 1.199116  [28000/50000]\n",
      "loss: 1.439556  [32000/50000]\n",
      "loss: 2.149318  [36000/50000]\n",
      "loss: 1.686028  [40000/50000]\n",
      "loss: 1.660669  [44000/50000]\n",
      "loss: 1.596102  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 1.742272 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.054003  [    0/50000]\n",
      "loss: 1.348103  [ 4000/50000]\n",
      "loss: 1.961236  [ 8000/50000]\n",
      "loss: 1.361232  [12000/50000]\n",
      "loss: 2.111283  [16000/50000]\n",
      "loss: 1.995502  [20000/50000]\n",
      "loss: 1.362173  [24000/50000]\n",
      "loss: 2.024738  [28000/50000]\n",
      "loss: 1.525625  [32000/50000]\n",
      "loss: 1.574604  [36000/50000]\n",
      "loss: 1.571628  [40000/50000]\n",
      "loss: 1.590730  [44000/50000]\n",
      "loss: 2.173471  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 1.725280 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.454076  [    0/50000]\n",
      "loss: 0.647148  [ 4000/50000]\n",
      "loss: 1.402244  [ 8000/50000]\n",
      "loss: 2.002636  [12000/50000]\n",
      "loss: 1.956055  [16000/50000]\n",
      "loss: 1.412370  [20000/50000]\n",
      "loss: 1.279941  [24000/50000]\n",
      "loss: 1.330296  [28000/50000]\n",
      "loss: 0.815499  [32000/50000]\n",
      "loss: 1.478408  [36000/50000]\n",
      "loss: 0.837180  [40000/50000]\n",
      "loss: 2.224396  [44000/50000]\n",
      "loss: 1.594646  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.716445 \n",
      "\n",
      "Done!\n",
      "10min 5s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(trainloader, net, loss_criterion, optimizer, device=device)\n",
    "    test_loop(testloader, net, loss_criterion, device=device)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net_100e_epoch-10.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(20, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note in the forward pass we have 'numbered'\n",
    "        # the x's just to make it easier to collect\n",
    "        # the code to print information about the network\n",
    "        # further on in the code.\n",
    "\n",
    "        # This is very bad code!!\n",
    "\n",
    "        conv1_out = self.conv1(x)\n",
    "        if self.verbose == 1:\n",
    "            print(\"Shape of conv1(x):\", conv1_out.shape)\n",
    "        x1 = self.pool1(F.relu(self.conv1(x)))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x1: {x1.shape}\")\n",
    "        x2 = self.pool2(F.relu(self.conv2(x1)))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x2: {x2.shape}\")\n",
    "        x3 = torch.flatten(x2, 1) # flatten all dimensions except batch\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x3: {x3.shape}\")\n",
    "        x4 = F.relu(self.fc1(x3))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x4: {x4.shape}\")\n",
    "        x5 = F.relu(self.fc2(x4))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x5: {x5.shape}\")\n",
    "        x6 = self.fc3(x5)\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x6: {x6.shape}\")\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5)\n",
    "        if self.verbose == 1:\n",
    "            print(self.conv1)\n",
    "\n",
    "        if self.verbose == 1:\n",
    "            # Write the code to print the shapes of the \n",
    "            # different stages in the CNN\n",
    "            # YOUR CODE HERE (Replace this and the following line with your code)\n",
    "            self.verbose -= 1\n",
    "        \n",
    "        return x6\n",
    "\n",
    "# with verbose=1 the 'debugging' code is printed only once \n",
    "# while running the forward pass multiple times.\n",
    "net = Net(verbose=0).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305791  [    0/50000]\n",
      "loss: 2.303194  [ 4000/50000]\n",
      "loss: 2.304226  [ 8000/50000]\n",
      "loss: 2.339684  [12000/50000]\n",
      "loss: 2.274350  [16000/50000]\n",
      "loss: 2.363783  [20000/50000]\n",
      "loss: 2.315879  [24000/50000]\n",
      "loss: 2.347579  [28000/50000]\n",
      "loss: 2.317857  [32000/50000]\n",
      "loss: 2.286231  [36000/50000]\n",
      "loss: 2.254052  [40000/50000]\n",
      "loss: 2.361178  [44000/50000]\n",
      "loss: 2.363640  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 2.306136 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.309299  [    0/50000]\n",
      "loss: 2.341513  [ 4000/50000]\n",
      "loss: 2.343765  [ 8000/50000]\n",
      "loss: 2.350560  [12000/50000]\n",
      "loss: 2.346117  [16000/50000]\n",
      "loss: 2.336022  [20000/50000]\n",
      "loss: 2.326306  [24000/50000]\n",
      "loss: 2.360272  [28000/50000]\n",
      "loss: 2.319454  [32000/50000]\n",
      "loss: 2.292853  [36000/50000]\n",
      "loss: 2.297202  [40000/50000]\n",
      "loss: 2.312392  [44000/50000]\n",
      "loss: 2.386729  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.306272 \n",
      "\n",
      "Done!\n",
      "2min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(trainloader, net, loss_criterion, optimizer, device=device)\n",
    "    test_loop(testloader, net, loss_criterion, device=device)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net_100e_channels-20.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note in the forward pass we have 'numbered'\n",
    "        # the x's just to make it easier to collect\n",
    "        # the code to print information about the network\n",
    "        # further on in the code.\n",
    "\n",
    "        # This is very bad code!!\n",
    "\n",
    "        conv1_out = self.conv1(x)\n",
    "        if self.verbose == 1:\n",
    "            print(\"Shape of conv1(x):\", conv1_out.shape)\n",
    "        x1 = self.pool1(F.relu(self.conv1(x)))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x1: {x1.shape}\")\n",
    "        x2 = self.pool2(F.relu(self.conv2(x1)))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x2: {x2.shape}\")\n",
    "        x3 = torch.flatten(x2, 1) # flatten all dimensions except batch\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x3: {x3.shape}\")\n",
    "        x4 = F.relu(self.fc1(x3))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x4: {x4.shape}\")\n",
    "        x5 = F.relu(self.fc2(x4))\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x5: {x5.shape}\")\n",
    "        x6 = self.fc3(x5)\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Shape x6: {x6.shape}\")\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        if self.verbose == 1:\n",
    "            print(self.conv1)\n",
    "\n",
    "        if self.verbose == 1:\n",
    "            # Write the code to print the shapes of the \n",
    "            # different stages in the CNN\n",
    "            # YOUR CODE HERE (Replace this and the following line with your code)\n",
    "            self.verbose -= 1\n",
    "        \n",
    "        return x6\n",
    "\n",
    "# with verbose=1 the 'debugging' code is printed only once \n",
    "# while running the forward pass multiple times.\n",
    "net = Net(verbose=0).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.321242  [    0/50000]\n",
      "loss: 2.314733  [ 4000/50000]\n",
      "loss: 2.294039  [ 8000/50000]\n",
      "loss: 2.299148  [12000/50000]\n",
      "loss: 2.282657  [16000/50000]\n",
      "loss: 2.268319  [20000/50000]\n",
      "loss: 2.298590  [24000/50000]\n",
      "loss: 2.310515  [28000/50000]\n",
      "loss: 2.302305  [32000/50000]\n",
      "loss: 2.318096  [36000/50000]\n",
      "loss: 2.338607  [40000/50000]\n",
      "loss: 2.353184  [44000/50000]\n",
      "loss: 2.293826  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 10.2%, Avg loss: 2.304593 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.324961  [    0/50000]\n",
      "loss: 2.281525  [ 4000/50000]\n",
      "loss: 2.328876  [ 8000/50000]\n",
      "loss: 2.313206  [12000/50000]\n",
      "loss: 2.313349  [16000/50000]\n",
      "loss: 2.310118  [20000/50000]\n",
      "loss: 2.332137  [24000/50000]\n",
      "loss: 2.361635  [28000/50000]\n",
      "loss: 2.318473  [32000/50000]\n",
      "loss: 2.288047  [36000/50000]\n",
      "loss: 2.266265  [40000/50000]\n",
      "loss: 2.310232  [44000/50000]\n",
      "loss: 2.313505  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 10.2%, Avg loss: 2.304529 \n",
      "\n",
      "Done!\n",
      "2min 4s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(trainloader, net, loss_criterion, optimizer, device=device)\n",
    "    test_loop(testloader, net, loss_criterion, device=device)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net_100e_kernel-3.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 5.1**</span>\n",
    "\n",
    "1. The obvious way to start improving your network is by training for more epochs. Only 2 epochs is really not much (but you need a fast computer or a GPU or a lot of patience). But you should be able to increase the number of epochs from 2 to 10 when you run the learning at night. What do you observe?\n",
    "\n",
    "1. Change the number of channels in the first data (image) block after the input image (a block with 3 channels) from 10 to 20 and train and run the network again. Does it improve the performance? Again start with 2 epochs of learning.\n",
    "\n",
    "1. Change the size of the convolution kernels in the `conv1` module from 5 to 3. Again start with 2 epochs of learning. What happens to the shapes of all blocks in the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPORT YOUR RESULTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you can learn faster or have a faster machine or have more time you could learn for a lot of epochs (say 100) and then plot the loss and accuracy for both the train and test set as a function of the number of epochs. That may provide you with evidence that the network is overfitting and thus that some regularization techniques might be needed. This is suggested by the webpages describing learning for the same dataset but then implemented in Keras/Tensorflow (see https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/). There the loss and accuracy as function of the number of epochs is shown as (in yellow the testset scores and in blue the trainset scores):\n",
    "<img src=\"cifar_overfitting.png\"\n",
    "     width=50%\n",
    "     alt=\"data flow graph\"\n",
    "     style=\"float: center;\" />\n",
    "     \n",
    "The network used was very much like the one that has been used in this lab.\n",
    "\n",
    "- In the above mentioned paper, using `DropOut` the problem of overfitting was succesfully mitigated. Try to implement such DropOut using pytorch to mitigate the problem of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
